{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.manifold.t_sne import _joint_probabilities, _kl_divergence\n",
    "from sklearn.manifold._utils import _binary_search_perplexity\n",
    "from scipy import linalg\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _joint_probabilities(distances, desired_perplexity):\n",
    "    distances = distances.astype(np.float32, copy=False)\n",
    "    conditional_P = _binary_search_perplexity(\n",
    "        distances, None, desired_perplexity, False)\n",
    "    P = conditional_P + conditional_P.T\n",
    "    sum_P = np.maximum(np.sum(P), MACHINE_EPSILON)\n",
    "    P = np.maximum(squareform(P) / sum_P, MACHINE_EPSILON)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_EPSILON = np.finfo(np.double).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "perplexity = 30\n",
    "learning_rate = 200\n",
    "n_iter = 1000\n",
    "min_grad_norm = 1e-7\n",
    "n_iter_without_progress = 300\n",
    "init = 'random'\n",
    "random_state = np.random.mtrand._rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X):\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    neighbors_nn = None\n",
    "    \n",
    "    distances = pairwise_distances(X, metric='euclidean', squared=True)\n",
    "    \n",
    "    P = _joint_probabilities(distances, perplexity)\n",
    "    \n",
    "    X_embedded = 1e-4 * random_state.randn(n_samples, n_components).astype(np.float32)\n",
    "    \n",
    "    degrees_of_freedom = max(n_components - 1, 1)\n",
    "    \n",
    "    return _tsne(P, degrees_of_freedom, n_samples, X_embedded=X_embedded, neighbors=neighbors_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kl_divergence(params, P, degrees_of_freedom, n_samples, n_components):\n",
    "    X_embedded = params.reshape(n_samples, n_components)\n",
    "    dist = pdist(X_embedded, \"sqeuclidean\")\n",
    "    dist /= degrees_of_freedom\n",
    "    dist += 1.\n",
    "    dist **= (degrees_of_freedom + 1.0) / -2.0\n",
    "    Q = np.maximum(dist / (2.0 * np.sum(dist)), MACHINE_EPSILON)\n",
    "    \n",
    "    kl_divergence = np.nan\n",
    "    \n",
    "    grad = np.ndarray((n_samples, n_components), dtype=params.dtype)\n",
    "    PQd = squareform((P - Q) * dist)\n",
    "    for i in range(n_samples):\n",
    "        grad[i] = np.dot(np.ravel(PQd[i], order='K'),\n",
    "                         X_embedded[i] - X_embedded)\n",
    "    grad = grad.ravel()\n",
    "    c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom\n",
    "    grad *= c\n",
    "\n",
    "    return kl_divergence, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tsne(P, degrees_of_freedom, n_samples, X_embedded, neighbors=None):\n",
    "    \n",
    "#     args = {\n",
    "#         'n_samples': n_samples,\n",
    "#         'n_components': n_components\n",
    "#     }\n",
    "\n",
    "    opt_args = {\n",
    "        'args': [P, degrees_of_freedom, n_samples, n_components]\n",
    "    }\n",
    "\n",
    "    params = X_embedded.ravel()\n",
    "    \n",
    "    obj_func = _kl_divergence\n",
    "    \n",
    "#     params, kl_divergence, it = _gradient_descent(obj_func, params, n_samples, n_components, degrees_of_freedom)\n",
    "    params, kl_divergence, it = _gradient_descent(obj_func, params, **opt_args)\n",
    "        \n",
    "    X_embedded = params.reshape(n_samples, n_components)\n",
    "\n",
    "    return X_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradient_descent(objective, p0, it=0, n_iter=1000,\n",
    "                      n_iter_check=1, n_iter_without_progress=300,\n",
    "                      momentum=0.8, learning_rate=200.0, min_gain=0.01,\n",
    "                      min_grad_norm=1e-7, args=None, kwargs=None):\n",
    "    \n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "    \n",
    "    p = p0.copy().ravel()\n",
    "    update = np.zeros_like(p)\n",
    "    gains = np.ones_like(p)\n",
    "    error = np.finfo(np.float).max\n",
    "    best_error = np.finfo(np.float).max\n",
    "    best_iter = i = it\n",
    "    \n",
    "    for i in range(it, n_iter):\n",
    "        check_convergence = (i + 1) % n_iter_check == 0\n",
    "        # only compute the error when needed\n",
    "#         kwargs['compute_error'] = check_convergence or i == n_iter - 1\n",
    "\n",
    "        error, grad = objective(p, *args, **kwargs)\n",
    "#         error, grad = objective(p, degrees_of_freedom, n_samples, n_components)\n",
    "\n",
    "        grad_norm = linalg.norm(grad)\n",
    "\n",
    "        inc = update * grad < 0.0\n",
    "        dec = np.invert(inc)\n",
    "        gains[inc] += 0.2\n",
    "        gains[dec] *= 0.8\n",
    "        np.clip(gains, min_gain, np.inf, out=gains)\n",
    "        grad *= gains\n",
    "        update = momentum * update - learning_rate * grad\n",
    "        p += update\n",
    "        \n",
    "      print(\"[t-SNE] Iteration %d: error = %.7f,\"\n",
    "                  \" gradient norm = %.7f\"\n",
    "                  % (i + 1, error, grad_norm))\n",
    "\n",
    "        \n",
    "        if error < best_error:\n",
    "                best_error = error\n",
    "                best_iter = i\n",
    "        elif i - best_iter > n_iter_without_progress:\n",
    "            break\n",
    "        \n",
    "        if grad_norm <= min_grad_norm:\n",
    "            break\n",
    "\n",
    "    return p, error, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
